{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title 🔍 Environment Validation (run before starting lab)\n",
        "import torch, os\n",
        "\n",
        "print(\"---- Environment Validation ----\")\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"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\"\n",
        "os.environ[\"HF_USER\"] = \"NiranjanIITPKD\"\n",
        "os.environ[\"SPACE_NAME\"] = \"cifar100-demo-space\"\n",
        "\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# Check required env vars\n",
        "for var in [\"HF_TOKEN\", \"HF_USER\", \"SPACE_NAME\"]:\n",
        "    val = os.environ.get(var)\n",
        "    print(f\"{var}:\", \" SET\" if val else \"NOT SET\")\n",
        "\n",
        "# Check if W&B API key configured (login handled separately)\n",
        "wandb_key = os.environ.get(\"WANDB_API_KEY\")\n",
        "print(\"WANDB_API_KEY:\", \" SET\" if wandb_key else \"⚠️ Not set (you'll log in interactively)\")\n",
        "\n",
        "# Check if dependencies installed\n",
        "try:\n",
        "    import wandb, gradio, huggingface_hub\n",
        "    print(\"Dependencies:  wandb, gradio, huggingface_hub imported successfully\")\n",
        "except Exception as e:\n",
        "    print(\"Dependencies: missing - please run install cell first\")\n",
        "    print(e)\n",
        "\n",
        "print(\"---------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg8ZN3EdiLi9",
        "outputId": "8d8bed89-69ec-4856-df17-0f2148b73d4b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Environment Validation ----\n",
            "GPU available: True\n",
            "GPU name: Tesla T4\n",
            "HF_TOKEN:  SET\n",
            "HF_USER:  SET\n",
            "SPACE_NAME:  SET\n",
            "WANDB_API_KEY:  SET\n",
            "Dependencies:  wandb, gradio, huggingface_hub imported successfully\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q wandb gradio huggingface_hub git-lfs\n"
      ],
      "metadata": {
        "id": "N1KDmKWBlyxn"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#@title 3) Securely set your Hugging Face token, username, and desired Space name\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "print(\"Paste your Hugging Face token when prompted. It will be hidden.\")\n",
        "hf_token = getpass(\"Hugging Face token: \")\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "\n",
        "# Edit these values (do NOT put the token here)\n",
        "hf_user = input(\"Enter your Hugging Face username (e.g. 'alice'): \").strip()\n",
        "space_name = input(\"Enter desired Space name (e.g. 'cifar100-demo-space'): \").strip()\n",
        "\n",
        "os.environ['HF_USER'] = hf_user\n",
        "os.environ['SPACE_NAME'] = space_name\n",
        "\n",
        "print(\"HF_TOKEN stored in runtime (hidden). HF_USER and SPACE_NAME saved in environment variables.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9RoOr1Il0ZQ",
        "outputId": "3d69e2f5-6c6c-4fe6-fb42-0d3b21135d77"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paste your Hugging Face token when prompted. It will be hidden.\n",
            "Hugging Face token: ··········\n",
            "Enter your Hugging Face username (e.g. 'alice'): NiranjanIITPKD\n",
            "Enter desired Space name (e.g. 'cifar100-demo-space'): cifar100-demo-space\n",
            "HF_TOKEN stored in runtime (hidden). HF_USER and SPACE_NAME saved in environment variables.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title 4) Authenticate Weights & Biases (W&B)\n",
        "import wandb\n",
        "print(\"Follow the prompt to authenticate W&B (this opens an input box).\")\n",
        "wandb.login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rrt9FRjXmKW8",
        "outputId": "953fe8c8-08fc-4df4-a546-0aaee9ed17c7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Follow the prompt to authenticate W&B (this opens an input box).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = getpass(\"Enter your W&B API key: \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn0mP2tfrl-u",
        "outputId": "0d9e76c1-3319-44f4-da12-10202e934715"
      },
      "execution_count": 53,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your W&B API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%bash\n",
        "cat > train.py <<'PY'\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import wandb\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--project\", type=str, default=\"cifar100-hf-demo\")\n",
        "    p.add_argument(\"--entity\", type=str, default=None)\n",
        "    p.add_argument(\"--epochs\", type=int, default=5)\n",
        "    p.add_argument(\"--batch-size\", type=int, default=128)\n",
        "    p.add_argument(\"--lr\", type=float, default=0.01)\n",
        "    return p.parse_args()\n",
        "\n",
        "def get_dataloaders(batch_size):\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4865, 0.4409),\n",
        "                             (0.2673, 0.2564, 0.2762)),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4865, 0.4409),\n",
        "                             (0.2673, 0.2564, 0.2762)),\n",
        "    ])\n",
        "    trainset = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=transform_train)\n",
        "    testset  = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=transform_test)\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    testloader  = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    return trainloader, testloader\n",
        "\n",
        "def train_one_epoch(model, device, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (inputs, targets) in enumerate(loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "    return running_loss / total, 100. * correct / total\n",
        "\n",
        "def evaluate(model, device, loader, criterion):\n",
        "    model.eval()\n",
        "    loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            l = criterion(outputs, targets)\n",
        "            loss += l.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return loss/total, 100.*correct/total\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    wandb.init(project=args.project, entity=args.entity, config=vars(args))\n",
        "    cfg = wandb.config\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    trainloader, testloader = get_dataloaders(cfg.batch_size)\n",
        "\n",
        "    model = resnet18(num_classes=100)\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=cfg.lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(cfg.epochs):\n",
        "        train_loss, train_acc = train_one_epoch(model, device, trainloader, optimizer, criterion)\n",
        "        test_loss, test_acc = evaluate(model, device, testloader, criterion)\n",
        "        wandb.log({\"epoch\": epoch+1, \"train_loss\": train_loss, \"train_acc\": train_acc,\n",
        "                   \"test_loss\": test_loss, \"test_acc\": test_acc})\n",
        "        print(f\"Epoch {epoch+1}: train_acc={train_acc:.2f} test_acc={test_acc:.2f}\")\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            os.makedirs(\"outputs\", exist_ok=True)\n",
        "            torch.save(model.state_dict(), \"outputs/model.pt\")\n",
        "            # log artifact\n",
        "            artifact = wandb.Artifact(\"resnet18-cifar100\", type=\"model\", metadata={\"test_acc\": best_acc})\n",
        "            artifact.add_file(\"outputs/model.pt\")\n",
        "            wandb.log_artifact(artifact)\n",
        "    print(\"Best test acc:\", best_acc)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n"
      ],
      "metadata": {
        "id": "G_2lhqYLmhHq"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title 6) Run training (edit the --entity to your W&B username/team)\n",
        "# Keep epochs small for demo (e.g., 3-5). Increase for better accuracy.\n",
        "!python train.py --project cifar100-hf-run --entity ir2023 --epochs 3 --batch-size 128\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSudxaWlmy7j",
        "outputId": "a3435f7c-cc18-4a86-c0b1-91f4299a1615"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m142502019\u001b[0m (\u001b[33mir2023\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m setting up run c6db3j1w (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run c6db3j1w (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/hf_space/wandb/run-20251022_171319-c6db3j1w\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mupbeat-puddle-2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ir2023/cifar100-hf-run\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ir2023/cifar100-hf-run/runs/c6db3j1w\u001b[0m\n",
            "100% 169M/169M [00:04<00:00, 38.0MB/s]\n",
            "Epoch 1: train_acc=11.29 test_acc=17.44\n",
            "Epoch 2: train_acc=19.82 test_acc=24.56\n",
            "Epoch 3: train_acc=24.93 test_acc=27.92\n",
            "Best test acc: 27.92\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mupbeat-puddle-2\u001b[0m at: \u001b[34mhttps://wandb.ai/ir2023/cifar100-hf-run/runs/c6db3j1w\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251022_171319-c6db3j1w/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python - <<'PY'\n",
        "import wandb, os, sys\n",
        "ENTITY = os.environ.get(\"WANDB_ENTITY\") or \"ir2023\"   # <-- edit if not set\n",
        "PROJECT = \"cifar100-hf-run\"\n",
        "ARTIFACT = \"resnet18-cifar100:latest\"\n",
        "api = wandb.Api()\n",
        "try:\n",
        "    artifact = api.artifact(f\"{ENTITY}/{PROJECT}/{ARTIFACT}\")\n",
        "    artifact.download(root=\"outputs\")\n",
        "    print(\"Downloaded artifact to outputs/\")\n",
        "except Exception as e:\n",
        "    print(\"Failed to download artifact:\", e)\n",
        "    sys.exit(1)\n",
        "PY\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTMMeWcmnFe8",
        "outputId": "dd2887fa-8a45-46f4-fcdd-c2c73a60e2e4"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded artifact to outputs/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb:   1 of 1 files downloaded.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > app.py <<'PY'\n",
        "import os, time, io\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18\n",
        "import gradio as gr\n",
        "\n",
        "MODEL_PATH = \"outputs/model.pt\"\n",
        "\n",
        "# If model not present, try download via W&B (requires WANDB_API_KEY secret in Space or env)\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    try:\n",
        "        import wandb\n",
        "        wandb_api_key = os.environ.get(\"WANDB_API_KEY\")\n",
        "        if wandb_api_key:\n",
        "            wandb.login(key=wandb_api_key)\n",
        "            api = wandb.Api()\n",
        "            artifact = api.artifact(os.environ.get(\"WANDB_ARTIFACT\", \"ir2023/cifar100-hf-run/resnet18-cifar100:latest\"))\n",
        "\n",
        "            artifact.download(root=\"outputs\")\n",
        "            print(\"Downloaded model via W&B artifact.\")\n",
        "        else:\n",
        "            print(\"WANDB_API_KEY not set; cannot download artifact.\")\n",
        "    except Exception as e:\n",
        "        print(\"Error downloading artifact via W&B:\", e)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = resnet18(num_classes=100)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32,32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4865, 0.4409),(0.2673,0.2564,0.2762))\n",
        "])\n",
        "\n",
        "def predict_image(img):\n",
        "    start = time.time()\n",
        "    x = transform(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(x)\n",
        "        probs = torch.nn.functional.softmax(out, dim=1)\n",
        "        conf, idx = probs.max(1)\n",
        "        class_idx = int(idx.item())\n",
        "        conf_val = float(conf.item())\n",
        "    latency = (time.time() - start) * 1000.0\n",
        "    return {\"class_idx\": class_idx, \"confidence\": round(conf_val,4), \"latency_ms\": round(latency,2)}\n",
        "\n",
        "iface = gr.Interface(fn=predict_image, inputs=gr.Image(type=\"pil\"), outputs=\"json\", title=\"CIFAR-100 demo\")\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch(server_name=\"0.0.0.0\", server_port=7860)\n",
        "PY\n"
      ],
      "metadata": {
        "id": "myOPF903naYZ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%bash\n",
        "cat > requirements.txt <<'REQ'\n",
        "torch\n",
        "torchvision\n",
        "gradio\n",
        "Pillow\n",
        "wandb\n",
        "huggingface_hub\n",
        "git-lfs\n",
        "REQ\n"
      ],
      "metadata": {
        "id": "XdtAEVMHncja"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%bash\n",
        "set -e\n",
        "# prepare local repo\n",
        "rm -rf hf_space || true\n",
        "mkdir hf_space\n",
        "cp app.py requirements.txt hf_space/\n",
        "cd hf_space\n",
        "\n",
        "git init\n",
        "git config user.email \"142502019@smail.iitpkd.ac.in\"\n",
        "git config user.name \"NiranjanIITPKD\"\n",
        "git lfs install\n",
        "\n",
        "python - <<'PY'\n",
        "from huggingface_hub import HfApi, Repository\n",
        "import os, sys\n",
        "token = os.environ.get(\"HF_TOKEN\")\n",
        "user = os.environ.get(\"HF_USER\")\n",
        "space = os.environ.get(\"SPACE_NAME\")\n",
        "if not token or not user or not space:\n",
        "    print(\"HF_TOKEN, HF_USER or SPACE_NAME not set. Aborting.\")\n",
        "    sys.exit(1)\n",
        "api = HfApi(token=token)\n",
        "\n",
        "repo_id = f\"{user}/{space}\"\n",
        "repo_url = api.create_repo(repo_id=repo_id, repo_type=\"space\",\n",
        "            space_sdk=\"gradio\",\n",
        "            exist_ok=True)\n",
        "print(\"Repo URL:\", repo_url)\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=\".\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"space\",\n",
        "    commit_message=\"Initial commit: CIFAR-100 Gradio app (no model)\"\n",
        ")\n",
        "\n",
        "print(\"Pushed to:\", repo_url)\n",
        "PY\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyjdrmiknhRD",
        "outputId": "1a31258a-8b75-4d38-cc42-7177171f166e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized empty Git repository in /content/hf_space/hf_space/.git/\n",
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Repo URL: https://huggingface.co/spaces/NiranjanIITPKD/cifar100-demo-space\n",
            "Pushed to: https://huggingface.co/spaces/NiranjanIITPKD/cifar100-demo-space\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "hint: Using 'master' as the name for the initial branch. This default branch name\n",
            "hint: is subject to change. To configure the initial branch name to use in all\n",
            "hint: of your new repositories, which will suppress this warning, call:\n",
            "hint: \n",
            "hint: \tgit config --global init.defaultBranch <name>\n",
            "hint: \n",
            "hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\n",
            "hint: 'development'. The just-created branch can be renamed via this command:\n",
            "hint: \n",
            "hint: \tgit branch -m <name>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xREfNn3zoBIx"
      },
      "execution_count": 60,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}